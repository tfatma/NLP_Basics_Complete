{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3911b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: c:\\Users\\fatma\\PreparationMaterialsInterviews\\PythonProjects\\Traditional_NLP\\.venv\\Scripts\\python.exe\n",
      "Python version: 3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:20:11) [MSC v.1938 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1462560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (25.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfc01646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: sentencepiece-0.1.99-cp39-cp39-win_amd64.whl is not a supported wheel on this platform.\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece\n",
    "# For some systems, you might need:\n",
    "# !pip install sentencepiece\n",
    "# !pip install git+https://github.com/google/sentencepiece.git\n",
    "# For Windows, try downloading a pre-built wheel\n",
    "!pip install https://github.com/google/sentencepiece/releases/download/v0.1.99/sentencepiece-0.1.99-cp39-cp39-win_amd64.whl\n",
    "\n",
    "# Replace cp39 with your Python version (check with: python --version)\n",
    "# For Python 3.8: cp38\n",
    "# For Python 3.10: cp310\n",
    "# For Python 3.11: cp311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f6d17ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (4.53.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from transformers) (0.33.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fatma\\preparationmaterialsinterviews\\pythonprojects\\traditional_nlp\\.venv\\lib\\site-packages (from requests->transformers) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb47b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e80c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=\"Dr. Smith's research on COVID-19 at U.S.A won't stop!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1178a02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " \"Smith's\",\n",
       " 'research',\n",
       " 'on',\n",
       " 'COVID-19',\n",
       " 'at',\n",
       " 'U.S.A',\n",
       " \"won't\",\n",
       " 'stop!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ffaf71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte Pair Encoding (BPE)\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05dcaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded tokens: ['Hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "tokenizer= Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer=Whitespace()\n",
    "trainer=BpeTrainer(vocab_size=1000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.train_from_iterator([\"Hello world\", \"hello there\", \"world peace\"], trainer=trainer)\n",
    "output=tokenizer.encode(\"Hello world\")\n",
    "print(f\"Encoded tokens: {output.tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ccad78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', '##ization', 'is', 'so', 'good', 'that', 'something', 'is', 'better', 'than', 'nothing', '.', 'i', 'am', 'not', 'sure', 'this', 'will', 'work', '.']\n",
      "[101, 19204, 3989, 2003, 2061, 2204, 2008, 2242, 2003, 2488, 2084, 2498, 1012, 1045, 2572, 2025, 2469, 2023, 2097, 2147, 1012, 102]\n",
      "20 22\n"
     ]
    }
   ],
   "source": [
    "# WordPiece Tokenization:  Used successfully in BERT and other models.\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text=\"tokenization is so good that something is better than nothing. I am not sure this will work.\"\n",
    "\n",
    "tokens=tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "token_id=tokenizer.encode(text, add_special_tokens=True)\n",
    "print(token_id)\n",
    "\n",
    "print(len(tokens), len(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20c3bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "032ae046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Piece Tokenization: Used in models like T5 and ALBERT\n",
    "\n",
    "# import sentencepiece as spm\n",
    "\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     input=\"path/to/your/text/file.txt\",\n",
    "#     model_prefix=\"sentencepiece_model\",\n",
    "#     vocab_size=10000,\n",
    "#     model_type=\"bpe\" # or \"unigram\" or \"char\" or \"word\"\n",
    "# )\n",
    "\n",
    "# sp= spm.SentencePieceProcessor(model_file=\"sentencepiece_model.\")\n",
    "\n",
    "# text = \"Hello world! This is tokenization.\"\n",
    "# tokens = sp.encode_as_pieces(text)\n",
    "# print(tokens)\n",
    "\n",
    "\n",
    "# from transformers import T5Tokenizer\n",
    "\n",
    "# tokenizer= T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# tokens=tokenizer.tokenize(\"Hello world\")\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47478aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
